---
title: "Sesión 7"
output:
  bookdown::html_document2:
    code_folding: hide 
    toc: true
    toc_float: true
#bibliography: references.bib
---

<center><img src="https://github.com/Estadistica-AnalisisPolitico/operations_onDFs/blob/main/Logo2025.png?raw=true" width="900"/></center>

<br>

Profesor:[Dr. José Manuel MAGALLANES REYES, Ph.D](http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/%22%20target=%22_blank) <br>

-   Profesor Principal del Departamento de Ciencias Sociales, Sección de Ciencia Política y Gobierno.

-   [Oficina 223](https://goo.gl/maps/xuGeG6o9di1i1y5m6) - Edificio CISEPA / ECONOMIA / CCSS

-   Telefono: (51) 1 - 6262000 anexo 4302

-   Correo Electrónico: [jmagallanes\@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe)

<a id='beginning'></a>

------------------------------------------------------------------------

<center> <header><h2>Análisis de Conglomerados</h2>  </header></center>

<center>
<a href="https://doi.org/10.5281/zenodo.7278483"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7278483.svg" alt="DOI"></a>
</center>
____



```{r klippy, echo=FALSE, include=TRUE}
#1
klippy::klippy(position = c('top', 'right'))
```

# Presentación

La idea intuitiva de conglomerar es poder organizar los casos (filas) en subconjuntos o grupos, tal que la similitud entre casos justifique que pertenezca a un grupo y no a otro.

En la sesión [anterior](https://estadistica-analisispolitico.github.io/Sesion6/), vimos como crear un índice numérico que resuma indicadores de un concepto. Utilicemos los datos ahí preprocesados:

Traigamos algunos datos de los paises del mundo para el ejemplo de esta sesión:
```{r}
rm(list = ls())

link_idhdemo="https://docs.google.com/spreadsheets/d/e/2PACX-1vRB4YNe2KdIrTmQUAMScYuWcA2ig8d5fKgJBQIlRPVKcryiurAY3dz4Dy8-fpa_MjqmPeTeYet1ggDR/pub?gid=1870508685&single=true&output=csv"

idhdemo=read.csv(link_idhdemo)
names(idhdemo)
```

# Transformación  de datos

Para este ejercicio sólo usaremos los componentes del IDH. La distribución de los componentes del IDH podemos verla en la Figura \@ref(fig:boxdemoOrig).

```{r boxdemoOrig, fig.cap="Distribución de los componentes del IDH"}

boxplot(idhdemo[,c(4:7)],horizontal = F,las=2,cex.axis = 0.5)

```

Como primera estrategia cambiemos sus rangos. Elijamos un rango  del 0 al 1, cuyo resultado se ve en la Figura \@ref(fig:)

```{r boxdemoRango,fig.cap="Distribución de los componentes del IDH con nuevo rango (0-1)"}

library(BBmisc)
boxplot(normalize(idhdemo[,c(4:7)],method='range',range=c(0,10)))
```

Una segunda estrategia sería tipificarla ^[Recuerda que la tipificación producirá variables con media igual a cero  y desviación típica igual a uno.]. El resultado se muestra en la Figura \@ref(fig:boxdemoZ).

```{r boxdemoZ,fig.cap="Distribución de los componentes del IDH tipificados"}
boxplot(normalize(idhdemo[,c(4:7)],method='standardize'))
```


Nos quedaremos con la segunda opción. 

```{r}
idhdemo[,c(4:7)]=normalize(idhdemo[,c(4:7)],method='standardize')
```

# Correlación

Veamos correlaciones entre estas variables tipificadas:

```{r}
cor(idhdemo[,c(4:7)])
```

Si hubiera alguna correlación negativa sería bueno invertir el rango, tal que el menor sea el mayor y viceversa. Esto no sucede aquí, por lo que no se hace ningún ajuste. 


# Preparación de los datos para la clusterización

No podemos usar la columna _Pais_ en la clusterización, pero tampoco debemos perderla, por lo que se recomienda usar esos nombres en lugar del nombre de fila.

```{r}
dataClus=idhdemo[,c(4:7)]
row.names(dataClus)=idhdemo$country
```

Ya con los datos en el objeto _dataClus_, calculemos la **matriz de  distancias** entre los casos (paises):

```{r}
library(cluster)
g.dist = daisy(dataClus, metric="gower")
```

Hay diversas maneras de calculas matrices de distancias entre casos. Cuando las variables son todas numéricas, es comun usar la [distancia _Euclideana_](https://en.wikipedia.org/wiki/Euclidean_distance)). Hay otras técnicas útiles como la [Mahattan](https://en.wikipedia.org/wiki/Taxicab_geometry) (revisar este [debate](https://datascience.stackexchange.com/questions/20075/when-would-one-use-manhattan-distance-as-opposite-to-euclidean-distance)). En nuestro caso, usaremos la [distancia Gower](https://www.linkedin.com/pulse/simplifying-gower-coefficient-vineet-tanna) útil cuando las variables (columnas) están de diversos tipos de escalas.


# Procesos de clusterización

Hay diversas estrategías de clusterización. Veremos dos de ellas en nuestro curso:

* La técnica de Partición 
* La técnica de Jerarquización
    - Jerarquización Aglomerativa
    - Jerarquización Divisiva


## <font color="blue">Estrategia de Partición</font>

Como su nombre lo indica, la estrategia de partición busca partir los casos en grupos. El algoritmo básico establece puntos que traten de ser el centro de los demás casos, tal que estos se separen. Claro está, que estos centros de atracción van moviéndose conforme los grupos se van formando, hasta que al final se han partido todos los casos. 

Hay diversos algoritmos que buscan una implementación de estos principios básicos. El más conocido es el de **K-medias**, pero para ciencias sociales tiene la desventaja que requiere que todas las variables sean numéricas, no siendo muy adecuado cuando haya presencia de  variables sean categóricas. La alternativa a las necesidades en ciencias sociales es la técnica de **k-medoides**. 


### Decidir cantidad de clusters:

La Figura \@ref(fig:gapPam) sirve para determinar la cantidad de clusters a solicitar (usando el estadístico _gap_).

```{r gapPam, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE, fig.cap="Clusters sugeridos para algoritmo PAM."}
## para PAM

library(factoextra)
fviz_nbclust(dataClus, pam,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F)
```



### Clusterizar via PAM:

La técnica de k-medoides se implementa en la función _pam_. Esta función retorna diversos valores, en este caso crearemos una columna con la etiqueta del cluster. Usemos la sugerencia de la  Figura \@ref(fig:gapPam), y hallamos:

```{r}
library(kableExtra)
set.seed(123)
res.pam=pam(g.dist,3,cluster.only = F)

#nueva columna
dataClus$pam=res.pam$cluster

# ver

head(dataClus,15)%>%kbl()%>%kable_styling()
```


### Evaluando el uso de PAM

Una manera práctica de ver el desempeño del algoritmo es calcular las _silhouettes_. Para el caso reciente, veamos la Figura \@ref(fig:silsPam).

```{r silsPam, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE, fig.cap="Evaluando resultados de PAM"}
fviz_silhouette(res.pam,print.summary = F)
```
La Figura \@ref(fig:silsPam) muestra barras, donde cada una es un país (caso). Mientras más alta la barra, la pertenencia a ese cluster es clara. La barra negativa indica un país mal clusterizado. Para este caso, estos serían los mal clusterizados:

```{r}
silPAM=data.frame(res.pam$silinfo$widths)
silPAM$country=row.names(silPAM)
poorPAM=silPAM[silPAM$sil_width<0,'country']%>%sort()
poorPAM
```


### Verificando etiqueta de clusters

Exploremos el promedio de cada cluster:

```{r}
aggregate(.~ pam, data=dataClus,mean)
```

El número asignado al cluster no tiene significado necesariamente, por lo que recomiendo hacer cálculos para dárselo. En este caso, las etiquetas ascienden al igual que el promedio, por lo que no es necesario recodificar la etiqueta.

Antes de continuar, guardemos la columna de PAM en la data integrada, y eliminemos la de __dataClus__.

```{r}
idhdemo$pamIDHpoor=idhdemo$country%in%poorPAM
idhdemo$pamIDH=as.ordered(dataClus$pam)
dataClus$pam=NULL
```



## <font color="blue">Estrategia Jerárquica</font>

La jerarquización busca clusterizar por etapas, hasta que todas las posibilidades de clusterizacion sean visible. Este enfoque tiene dos familias de algoritmos:

* Aglomerativos
* Divisivos


### <font color="red">Estrategia Aglomerativa</font>


En esta estrategia se parte por considerar cada caso (fila) como un cluster, para de ahi ir creando miniclusters hasta que todos los casos sean un solo cluster. El proceso va mostrando qué tanto _esfuerzo_ toma juntar los elementos cluster tras cluster.


#### Decidir _linkages_

Aunque se tiene la distancia entre elementos, tenemos que decidir como se irá calculando la distancia entre los clusters que se van formando (ya no son casos individuales). Los tres mas simples metodos:

* Linkage tipo <a href="https://www.youtube.com/embed/RdT7bhm1M3E" target="_blank">SINGLE</a>.

* Linkage tipo <a href="https://www.youtube.com/embed/Cy3ci0Vqs3Y" target="_blank">COMPLETE</a>.

* Linkage tipo <a href="https://www.youtube.com/embed/T1ObCUpjq3o" target="_blank">AVERAGE</a>


Otro metodo adicional, y muy eficiente, es el de **Ward**. Al final, lo que necesitamos saber cual de ellos nos entregará una mejor propuesta de clusters. Usemos este último para nuestro caso.


#### Decidir cantidad de Clusters

La Figura \@ref(fig:gapAgn) sirve para determinar la cantidad de clusters a solicitar (usando el estadístico _gap_).

```{r gapAgn, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE,, fig.cap="Clusters sugeridos para algoritmo AGNES."}
## PARA JERARQUICO

fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
```

#### Clusterizar vía AGNES

La función **hcut** es la que usaremos para el método jerarquico, y el algoritmo aglomerativo se emplea usando **agnes**. El linkage será **ward** (aquí _ward.D_):

```{r, warning=FALSE, message=FALSE, warning=FALSE, message=FALSE}
set.seed(123)
library(factoextra)

res.agnes<- hcut(g.dist, k = 3,hc_func='agnes',hc_method = "ward.D")

dataClus$agnes=res.agnes$cluster

# ver

head(dataClus,15)%>%kbl()%>%kable_styling()

```

El **dendograma** de la Figura \@ref(fig:dendo1) nos muestra el proceso de conglomeración AGNES:

```{r dendo1, warning=FALSE, message=FALSE, warning=FALSE, message=FALSE, fig.cap="Dendograma de AGNES"}
# Visualize
fviz_dend(res.agnes, cex = 0.7, horiz = T,main = "")
```

El eje 'Height' nos muestra el "costo" de conglomerar: mientras más corta la distancia mayor similitud y la conglomeracion es más rápida.



#### Evaluando el uso de AGNES

La Figura \@ref(fig:silsAgn) nos muestra las _silhouettes_ para AGNES.

```{r silsAgn, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE, fig.cap="Evaluando resultados de AGNES"}

fviz_silhouette(res.agnes,print.summary = F)
```

Nótese que también se presentan valores mal clusterizados. Los identificados son estos:

```{r}
silAGNES=data.frame(res.agnes$silinfo$widths)
silAGNES$country=row.names(silAGNES)
poorAGNES=silAGNES[silAGNES$sil_width<0,'country']%>%sort()
poorAGNES
```

#### Verificando etiqueta de clusters

Exploremos el promedio de cada cluster:

```{r}
aggregate(.~ agnes, data=dataClus,mean)
```

Estas etiquetas no necesitan recodificación tampoco. Guardemos la columna de AGNES en la data integrada, y eliminemosla de __dataClus__.

```{r}
idhdemo$agnesIDHpoor=idhdemo$country%in%poorAGNES
idhdemo$agnesIDH=as.ordered(dataClus$agnes)
dataClus$agnes=NULL
```

#### Comparando

Veamos qué tanto se parece a la clasificación jerarquica a la de partición:

```{r, warning=FALSE, message=FALSE, warning=FALSE, message=FALSE}
# verificar recodificacion
table(idhdemo$pamIDH,idhdemo$agnesIDH,dnn = c('Particion','Aglomeracion'))
```



### <font color="red">Estrategia Divisiva</font>


Esta estrategia comienza con todos los casos como un gran cluster; para de ahi dividir en clusters más pequeños.

#### Decidir Cantidad de Clusters

La Figura \@ref(fig:gapDia) sirve para determinar la cantidad de clusters a solicitar (usando el estadístico _gap_).

```{r gapDia, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE,, fig.cap="Clusters sugeridos para algoritmo DIANA"}
## PARA JERARQUICO

fviz_nbclust(dataClus, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "diana")
```

#### Clusterizar vía DIANA


La función **hcut** es la que usaremos para el método jerarquico, y el algoritmo divisivo se emplea usando **diana**. Aquí una muestra del resultado:


```{r, warning=FALSE, message=FALSE, warning=FALSE, message=FALSE}
set.seed(123)
res.diana <- hcut(g.dist, k = 4,hc_func='diana')
dataClus$diana=res.diana$cluster
# veamos
head(dataClus,15)%>%kbl%>%kable_styling()
```


El **dendograma** de la Figura \@ref(fig:dendo2) nos muestra el proceso de conglomeración AGNES:

```{r dendo2, warning=FALSE, message=FALSE, warning=FALSE, message=FALSE,fig.cap="Dendograma de DIANA"}
# Visualize
fviz_dend(res.diana, cex = 0.7, horiz = T, main = "")
```

#### Evaluando el uso de DIANA

La Figura \@ref(fig:silsDia) nos muestra las _silhouettes_ para DIANA.

```{r silsDia, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE, fig.cap="Evaluando resultados de DIANA"}
fviz_silhouette(res.diana,print.summary = F)
```

Nótese que también se presentan valores mal clusterizados. Los identificados son estos:

```{r}
silDIANA=data.frame(res.diana$silinfo$widths)
silDIANA$country=row.names(silDIANA)
poorDIANA=silDIANA[silDIANA$sil_width<0,'country']%>%sort()
poorDIANA

```

#### Verificando Etiqueta

Exploremos el promedio de cada cluster:

```{r}
aggregate(.~ diana, data=dataClus,mean)
```


Aquí vemos que las etiquetas no muestran un orden. Este sería el orden:

```{r}
original=aggregate(.~ diana, data=dataClus,mean)
original[order(original$hdiLife),]
```

Esas posiciones hay que usarlas para recodificar:

```{r}
dataClus$diana=dplyr::recode(dataClus$diana, `1` = 1, `4`=2,`2`=3,`3`=4)
```


Guardemos la columna de DIANA en la data integrada, y eliminemosla de __dataClus__.

```{r}
idhdemo$dianaIDHpoor=idhdemo$country%in%poorDIANA
idhdemo$dianaIDH=as.ordered(dataClus$diana)
dataClus$diana=NULL
```

# Visualización comparativa

Vamos a usar la matriz de distancia para darle a cada país una coordenada, tal que la distancia entre esos paises se refleje en sus posiciones. Eso requiere una técnica que _proyecte_ las dimensiones originales en un plano _bidimensional_. Para ello usaremos la técnica llamada **escalamiento multidimensional**. Veams algunas coordenadas.

```{r, warning=FALSE, message=FALSE, eval=TRUE}
# k es la cantidad de dimensiones
proyeccion = cmdscale(g.dist, k=2,add = T) 
head(proyeccion$points,20)
```

Habiendo calculado la proyeccción, recuperemos las coordenadas del mapa del mundo basado en nuestras dimensiones nuevas:

```{r, warning=FALSE, message=FALSE, eval=TRUE}
# data frame prep:
idhdemo$dim1 <- proyeccion$points[,1]
idhdemo$dim2 <- proyeccion$points[,2]
```

Aquí puedes ver el mapa:

```{r, warning=FALSE, message=FALSE, eval=TRUE,fig.height=6}
library(ggrepel)
base= ggplot(idhdemo,aes(x=dim1, y=dim2,label=row.names(dataClus))) 
base + geom_text_repel(size=3, max.overlaps = 50,min.segment.length = unit(0, 'lines'))

```

Coloreemos el mapa anterior segun el cluster al que corresponden. 


## Gráfica de PAM


```{r pamColor, warning=FALSE, message=FALSE,fig.height=6, fig.cap="Conglomerados PAM en Mapa Bidimensonal de países"}

# solo paises mal clusterizados
PAMlabels=ifelse(idhdemo$pamIDHpoor,idhdemo$country,'')

#base
base= ggplot(idhdemo,aes(x=dim1, y=dim2))  +
    scale_color_brewer(type = 'qual',palette ='Dark2'  ) + labs(subtitle = "Se destacan los países mal clusterizados")

pamPlot=base + geom_point(size=3, 
                          aes(color=pamIDH))  + 
        labs(title = "PAM") 
# hacer notorios los paises mal clusterizados
pamPlot + geom_text_repel(size=4,
                          aes(label=PAMlabels),
                          max.overlaps = 50,
                          min.segment.length = unit(0, 'lines'))
```

## Gráfica de AGNES

```{r agnColor, warning=FALSE, message=FALSE,fig.height=6, fig.cap="Conglomerados AGNES en Mapa Bidimensonal de países"}
# solo paises mal clusterizados
AGNESlabels=ifelse(idhdemo$agnesIDHpoor,idhdemo$country,'')

agnesPlot=base + geom_point(size=3, 
                            aes(color=as.factor(agnesIDH))) +
          labs(title = "AGNES") 
# hacer notorios los paises mal clusterizados
agnesPlot + geom_text_repel(size=4,
                            aes(label=AGNESlabels),
                            max.overlaps = 50,
                            min.segment.length = unit(0, 'lines'))
```

## Gráfica de DIANA

```{r diaColor, warning=FALSE, message=FALSE,fig.height=6, fig.cap="Conglomerados DIANA en Mapa Bidimensonal de países"}

# solo paises mal clusterizados
DIANAlabels=ifelse(idhdemo$dianaIDHpoor,idhdemo$country,'')

dianaPlot=base + geom_point(size=3,
                            aes(color=dianaIDH)) + 
          labs(title = "DIANA")

# hacer notorios los paises mal clusterizados
dianaPlot + geom_text_repel(size=4,
                            aes(label=DIANAlabels), 
                            max.overlaps = 50,
                            min.segment.length = unit(0, 'lines'))
```

**Nota** que en estas técnicas (partición y jerarquica) todo elemento termina siendo parte de un cluster.


